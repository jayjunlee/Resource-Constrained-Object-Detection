{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pytorch modules and other ML/DL modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchsummary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# other modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "# data augmentation module\n",
    "import data_augmentation as aug"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Resize raw images to processed images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not os.path.exists('./dataset/processed'):\n",
    "    os.mkdir('./dataset/processed')\n",
    "    raw_images = glob.glob('./dataset/raw/*.jpg')\n",
    "    for img in raw_images:\n",
    "        raw = Image.open(img)\n",
    "        processed = raw.resize((640,480))\n",
    "        name = os.path.basename(img)\n",
    "        processed.save('./dataset/processed/'+name)\n",
    "else:\n",
    "    print(\"640 x 480 sized images generated already in ./dataset/processed directory.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Read in processed image path and labels into dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_image_path = Path('./dataset/processed')\n",
    "processed_labels = pd.read_csv('./dataset/label/labels.csv')\n",
    "colour_dict = {'blue':0, 'green':1, 'yellow':2, 'red':3, 'pink':4}\n",
    "dict_colour = [\"blue\",\"green\",\"yellow\",\"red\",\"pink\"]\n",
    "processed_labels['colour'] = np.array([ colour_dict[processed_labels['colour'][x]] for x in range(len(processed_labels['colour'])) ])\n",
    "processed_labels['filepath'] = processed_labels['filepath'].apply(lambda x: Path(str(processed_image_path) + '/' + x))\n",
    "\n",
    "bboxes = [ np.array([processed_labels['bbox_x'][index], processed_labels['bbox_y'][index], processed_labels['bbox_w'][index], processed_labels['bbox_h'][index]], dtype=np.float32) for index, row in processed_labels.iterrows() ]\n",
    "processed_labels['bbox'] = bboxes\n",
    "processed_labels = processed_labels.drop(['img_width','img_height'], axis=1)\n",
    "processed_labels.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Save and add extra columns of smaller images of size 320 x 240 onto dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_paths = []\n",
    "new_bbs = []\n",
    "if not os.path.exists('./dataset/resized'):\n",
    "    os.mkdir('./dataset/resized')\n",
    "train_path_resized = Path('./dataset/resized')\n",
    "for index, row in processed_labels.iterrows():\n",
    "    new_path,new_bb = aug.resize_image_bb(row['filepath'], train_path_resized, aug.create_bb_array(row.values),240)\n",
    "    new_paths.append(new_path)\n",
    "    new_bbs.append(new_bb)\n",
    "processed_labels['new_path'] = new_paths\n",
    "processed_labels['new_bb'] = new_bbs\n",
    "processed_labels.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Train and validation datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = processed_labels[['filepath', 'bbox', 'new_path','new_bb']]\n",
    "Y = processed_labels['colour']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "class Ball_Dataset_640x480(Dataset):\n",
    "    def __init__(self, path, bbox, colour, transforms=False):\n",
    "        self.path = path.values\n",
    "        self.colour = colour.values\n",
    "        self.bbox = bbox.values\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.path[index]\n",
    "        colour = self.colour[index]\n",
    "        bbox = self.bbox[index]\n",
    "        x, bbox = aug.transformsXY(filepath, self.bbox[index], self.transforms)\n",
    "        image = torch.from_numpy(np.rollaxis(x, 2))\n",
    "        return image, bbox, np.array(colour)\n",
    "\n",
    "class Ball_Dataset_320x240(Dataset):\n",
    "    def __init__(self, path, bbox, colour, transforms=False):\n",
    "        self.path = path.values\n",
    "        self.colour = colour.values\n",
    "        self.bbox = bbox.values\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.path[index]\n",
    "        colour = self.colour[index]\n",
    "        bbox = self.bbox[index]\n",
    "        x, bbox = aug.transformsXY(filepath, self.bbox[index], self.transforms, 240) # image size specified by height\n",
    "        image = torch.from_numpy(np.rollaxis(x, 2))\n",
    "        return image, bbox, np.array(colour)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = Ball_Dataset_640x480(X_train['filepath'], X_train['bbox'], Y_train, transforms=True)\n",
    "test_dataset = Ball_Dataset_640x480(X_test['filepath'], X_test['bbox'], Y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_dataset_320x240 = Ball_Dataset_320x240(X_train['new_path'], X_train['new_bb'], Y_train, transforms=True)\n",
    "test_dataset_320x240 = Ball_Dataset_320x240(X_test['new_path'], X_test['new_bb'], Y_test)\n",
    "train_dataloader_320x240 = DataLoader(train_dataset_320x240, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_320x240 = DataLoader(test_dataset_320x240, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Different ways of plotting images by accessing dataframe values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "axes=[]\n",
    "fig,ax=plt.subplots(1,3,squeeze=False, figsize=(30,5))\n",
    "i=1\n",
    "while (i != 6):\n",
    "    axes.append( fig.add_subplot(1,5,i) )\n",
    "    if i == 1:\n",
    "        aug.draw_bbox(aug.read_image(X_train.iloc[0][0]), X_train.iloc[0][1], dict_colour[Y_train.iloc[0]])\n",
    "    elif i == 2:\n",
    "        aug.draw_bbox(aug.read_image(str(X_train['new_path'][3])), X_train['new_bb'][3], dict_colour[Y_train.iloc[3]]) # resized image (same image as below)\n",
    "    elif i == 3:\n",
    "        aug.draw_bbox(aug.read_image(str(X_train['filepath'][3])), X_train['bbox'][3], dict_colour[Y_train.iloc[3]])\n",
    "    elif i == 4:\n",
    "        index = 6\n",
    "        im, bb = aug.transformsXY(str(X_train['new_path'][index]),X_train['new_bb'][index],True,240)  # augmented image\n",
    "        print(X_train['new_path'][index])\n",
    "        print(im.shape)\n",
    "        aug.draw_bbox(im, bb, dict_colour[Y_train[index]])\n",
    "    else:\n",
    "        index = 6\n",
    "        im2, bb2 = aug.transformsXY(str(X_train['new_path'][index]),X_train['new_bb'][index],False,240) # unaugmented image\n",
    "        print(X_train['new_path'][index])\n",
    "        print(im2.shape)\n",
    "        aug.draw_bbox(im2, bb2, dict_colour[Y_train[index]])\n",
    "    i+=1\n",
    "[axi.set_axis_off() for axi in ax.ravel()]\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. CNN architecture, train and evaluate functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=9,kernel_size=11,stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=9,out_channels=18,kernel_size=5,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(972,256,bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.colour = nn.Linear(256,5)\n",
    "        self.bbox = nn.Linear(256,4)\n",
    "    def forward(self,x):\n",
    "        output_conv = self.conv2(self.conv1(x))\n",
    "        output_flat = output_conv.view(output_conv.shape[0],-1)\n",
    "        output_dense = self.dense(output_flat)\n",
    "        output_colour = self.colour(output_dense)\n",
    "        output_bbox = self.bbox(output_dense)\n",
    "        return output_bbox, output_colour\n",
    "\n",
    "def train(model, optimizer, train_dataloader, test_dataloader, epochs=30, C=1000):\n",
    "    index = 0\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for image, bbox, colour in train_dataloader:\n",
    "            batch = colour.shape[0]\n",
    "            output_bbox, output_colour = model(image) \n",
    "            loss_colour = nn.functional.cross_entropy(output_colour, colour, reduction=\"sum\")\n",
    "            loss_bbox = nn.functional.l1_loss(output_bbox, bbox, reduction=\"none\").sum(1)\n",
    "            loss_bbox = loss_bbox.sum()\n",
    "            loss = loss_colour + loss_bbox/C\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            index += 1\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "        train_loss = sum_loss/total*100\n",
    "        test_loss, test_accuracy = evaluate(model, test_dataloader, C)\n",
    "        print(\"EPOCH %d | train loss %.3f test loss %.3f colour accuracy %.3f\" % (i, train_loss, test_loss, test_accuracy))\n",
    "    return sum_loss/total\n",
    "\n",
    "def evaluate(model, test_dataloader, C=1000):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0 \n",
    "    for image, bbox, colour in test_dataloader:\n",
    "        batch = colour.shape[0]\n",
    "        output_bbox, output_colour = model(image)\n",
    "        _, output_colour_num = torch.max(output_colour,1) # Tensor, LongTensor = torch.max(input, 1)\n",
    "        loss_colour = nn.functional.cross_entropy(output_colour, colour, reduction=\"sum\")\n",
    "        loss_bbox = nn.functional.l1_loss(output_bbox, bbox, reduction=\"none\").sum(1)\n",
    "        loss_bbox = loss_bbox.sum()\n",
    "        loss = loss_colour + loss_bbox/C\n",
    "        correct += output_colour_num.eq(colour).sum().item()\n",
    "        sum_loss += loss.item()\n",
    "        total += batch\n",
    "    return sum_loss/total*100, correct/total*100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Train CNN model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model = CNN()\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters, lr=0.0003)\n",
    "train(model,optimizer,train_dataloader_320x240,test_dataloader_320x240,epochs=1000,C=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Save trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model.state_dict(), \"./trained_model/model_cnn_320x240.pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Load trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = CNN() # a torch.nn.Module object\n",
    "model.load_state_dict(torch.load(\"./trained_model/model_cnn_320x240.pt\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Evaluate trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "count = 0\n",
    "for filepath, bbox, colour in zip(X_train['new_path'].iteritems(), X_train['new_bb'].iteritems(), Y_train.iteritems()):\n",
    "\n",
    "    # true\n",
    "    filepath = filepath[1]\n",
    "    bbox = bbox[1].astype(np.int16)\n",
    "    colour = colour[1]\n",
    "    dict_colour = [\"blue\",\"green\",\"yellow\",\"red\",\"pink\"]\n",
    "    colour = dict_colour[colour]\n",
    "\n",
    "    # predicted\n",
    "    x = cv2.cvtColor(cv2.imread(str(filepath)).astype(np.float32), cv2.COLOR_BGR2RGB) / 255\n",
    "    img = torch.from_numpy(np.rollaxis(x, 2))[None,]\n",
    "    pred_bbox, pred_colour = model(img)\n",
    "    pred_bbox = pred_bbox.data[0].numpy().astype(np.int16)\n",
    "    _, pred_colour = torch.max(pred_colour,1)\n",
    "    pred_colour = dict_colour[pred_colour]\n",
    "\n",
    "    loss_bbox = int((np.square(bbox - pred_bbox)).mean(axis=None))\n",
    "\n",
    "    if colour != pred_colour or loss_bbox >= 200:\n",
    "        print(f\"{i} [{filepath}] =>\", \"T:\", bbox, colour, \"|| P:\", pred_bbox, pred_colour, \"|| loss:\", loss_bbox)\n",
    "        count += 1\n",
    "    i+=1\n",
    "\n",
    "for filepath, bbox, colour in zip(X_test['new_path'].iteritems(), X_test['new_bb'].iteritems(), Y_test.iteritems()):\n",
    "\n",
    "    # true\n",
    "    filepath = filepath[1]\n",
    "    bbox = bbox[1].astype(np.int16)\n",
    "    colour = colour[1]\n",
    "    dict_colour = [\"blue\",\"green\",\"yellow\",\"red\",\"pink\"]\n",
    "    colour = dict_colour[colour]\n",
    "\n",
    "    # predicted\n",
    "    x = cv2.cvtColor(cv2.imread(str(filepath)).astype(np.float32), cv2.COLOR_BGR2RGB) / 255\n",
    "    img = torch.from_numpy(np.rollaxis(x, 2))[None,]\n",
    "    pred_bbox, pred_colour = model(img)\n",
    "    pred_bbox = pred_bbox.data[0].numpy().astype(np.int16)\n",
    "    _, pred_colour = torch.max(pred_colour,1)\n",
    "    pred_colour = dict_colour[pred_colour]\n",
    "\n",
    "    loss_bbox = int((np.square(bbox - pred_bbox)).mean(axis=None))\n",
    "\n",
    "    if colour != pred_colour or loss_bbox >= 200:\n",
    "        print(f\"{i} [{filepath}] =>\", \"T:\", bbox, colour, \"|| P:\", pred_bbox, pred_colour, \"|| loss:\", loss_bbox)\n",
    "        count += 1\n",
    "    i+=1\n",
    "print(count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Actual performance of trained model on image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    index = 16\n",
    "    try:\n",
    "        im, bb = aug.transformsXY(str(X_test['new_path'][index]),X_test['new_bb'][index],False,240 )\n",
    "        print(X_test['new_path'][index])\n",
    "        x = cv2.cvtColor(cv2.imread(str(X_test['new_path'][index])).astype(np.float32), cv2.COLOR_BGR2RGB) / 255\n",
    "        img = torch.from_numpy(np.rollaxis(x, 2))[None,]\n",
    "        pred_bbox, pred_colour = model(img)\n",
    "        _, pred_colour = torch.max(pred_colour,1)\n",
    "        aug.draw_bbox(im, pred_bbox.data[0], dict_colour[pred_colour])\n",
    "    except KeyError:\n",
    "        im, bb = aug.transformsXY(str(X_train['new_path'][index]),X_train['new_bb'][index],False,240 )\n",
    "        print(X_train['new_path'][index])\n",
    "\n",
    "        x = cv2.cvtColor(cv2.imread(str(X_train['new_path'][index])).astype(np.float32), cv2.COLOR_BGR2RGB) / 255\n",
    "        img = torch.from_numpy(np.rollaxis(x, 2))[None,]\n",
    "        pred_bbox, pred_colour = model(img)\n",
    "        _, pred_colour = torch.max(pred_colour,1)\n",
    "        aug.draw_bbox(im, pred_bbox.data[0], dict_colour[pred_colour])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12. Display full dataframe of train and validation sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "    display(X_train.sort_index())\n",
    "    display(X_test.sort_index())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13. Visualise trained convolutional layer filters (feature maps)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kernels = model.conv1[0].weight.detach().clone()\n",
    "print(kernels.size())\n",
    "kernels = kernels - kernels.min()\n",
    "kernels = kernels / kernels.max()\n",
    "filter_img = torchvision.utils.make_grid(kernels, nrow = 3)\n",
    "plt.imshow(filter_img.permute(1, 2, 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kernels = model.conv2[0].weight.detach().clone()\n",
    "print(kernels.size())\n",
    "kernels = kernels[:,0,:,:].unsqueeze(dim=1)\n",
    "print(kernels.size())\n",
    "kernels = kernels - kernels.min()\n",
    "kernels = kernels / kernels.max()\n",
    "filter_img = torchvision.utils.make_grid(kernels, nrow = 4)\n",
    "print(filter_img.shape)\n",
    "print(filter_img.permute(1, 2, 0).shape)\n",
    "plt.imshow(filter_img.permute(1,2,0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14. Print out overall CNN architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 15. Summary of CNN architecture regarding its parameters and size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torchsummary.summary(model,(3,240,320))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}